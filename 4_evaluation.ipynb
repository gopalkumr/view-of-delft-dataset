{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation Example\n",
    "\n",
    "This example provides an insight on how to evaluate a model on the dataset. The same evaluation procedure is the basis of the upcoming leaderboard.\n",
    "For the evaluation the 'vod.evaluation' module is used. The evaluation module provides a number of metrics that can be used to evaluate a model. The metrics are:\n",
    "- Per class AP for the entire annotated area\n",
    "- Per class AP for the driving corridor\n",
    "- Per clas AOS for the entire annotated area\n",
    "- Per class AOS for the driving corridor\n",
    "\n",
    "The evaluation procedure can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating kitti by default\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m evaluation \u001b[39m=\u001b[39m Evaluation(test_annotation_file\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mE:/vod/view_of_delft_PUBLIC\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      8\u001b[0m \u001b[39m# Using the evaluate method, the model can be evaluated on the detection labels.\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m results \u001b[39m=\u001b[39m evaluation\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m     10\u001b[0m     \u001b[39m#result_path=os.path.join('example_set', 'detection'),\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m     result_path\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m'\u001b[39;49m\u001b[39mE:/vod/view_of_delft_PUBLIC/\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdetection\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     12\u001b[0m     current_class\u001b[39m=\u001b[39;49m[\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m])\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mResults: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEntire annotated area: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCar: \u001b[39m\u001b[39m{\u001b[39;00mresults[\u001b[39m'\u001b[39m\u001b[39mentire_area\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mCar_3d_all\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmAP: \u001b[39m\u001b[39m{\u001b[39;00m(results[\u001b[39m'\u001b[39m\u001b[39mroi\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mCar_3d_all\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39mresults[\u001b[39m'\u001b[39m\u001b[39mroi\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mPedestrian_3d_all\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39mresults[\u001b[39m'\u001b[39m\u001b[39mroi\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mCyclist_3d_all\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39m3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m       )\n",
      "File \u001b[1;32me:\\github\\view-of-delft-dataset\\view-of-delft-dataset\\vod\\evaluation\\evaluate.py:44\u001b[0m, in \u001b[0;36mEvaluation.evaluate\u001b[1;34m(self, result_path, current_class, score_thresh)\u001b[0m\n\u001b[0;32m     41\u001b[0m gt_annotations \u001b[39m=\u001b[39m kitti\u001b[39m.\u001b[39mget_label_annotations(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_annotation_file, val_image_ids)\n\u001b[0;32m     43\u001b[0m evaluation_result \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 44\u001b[0m evaluation_result\u001b[39m.\u001b[39mupdate(get_official_eval_result(gt_annotations, dt_annotations, current_class, custom_method\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m))\n\u001b[0;32m     45\u001b[0m evaluation_result\u001b[39m.\u001b[39mupdate(get_official_eval_result(gt_annotations, dt_annotations, current_class, custom_method\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m))\n\u001b[0;32m     47\u001b[0m \u001b[39mreturn\u001b[39;00m evaluation_result\n",
      "File \u001b[1;32me:\\github\\view-of-delft-dataset\\view-of-delft-dataset\\vod\\evaluation\\kitti_official_evaluate.py:741\u001b[0m, in \u001b[0;36mget_official_eval_result\u001b[1;34m(gt_annotations, dt_annotations, current_classes, pr_detail_dict, custom_method)\u001b[0m\n\u001b[0;32m    738\u001b[0m             compute_aos \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    739\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 741\u001b[0m mAPbbox, mAPbev, mAP3d, mAPaos, mAPbbox_R40, mAPbev_R40, mAP3d_R40, mAPaos_R40 \u001b[39m=\u001b[39m do_eval(\n\u001b[0;32m    742\u001b[0m     gt_annotations, dt_annotations, current_classes, min_overlaps, compute_aos, pr_detail_dict\u001b[39m=\u001b[39;49mpr_detail_dict,\n\u001b[0;32m    743\u001b[0m     custom_method\u001b[39m=\u001b[39;49mcustom_method)\n\u001b[0;32m    745\u001b[0m ret_dict \u001b[39m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m \u001b[39mfor\u001b[39;00m j, curcls \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(current_classes):\n\u001b[0;32m    747\u001b[0m     \u001b[39m# mAP threshold array: [num_min_overlap, metric, class]\u001b[39;00m\n\u001b[0;32m    748\u001b[0m     \u001b[39m# mAP result: [num_class, num_diff, num_min_overlap]\u001b[39;00m\n",
      "File \u001b[1;32me:\\github\\view-of-delft-dataset\\view-of-delft-dataset\\vod\\evaluation\\kitti_official_evaluate.py:647\u001b[0m, in \u001b[0;36mdo_eval\u001b[1;34m(gt_annotations, dt_annotations, current_classes, min_overlaps, compute_aos, pr_detail_dict, custom_method)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[39mif\u001b[39;00m custom_method \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    645\u001b[0m     difficulties \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 647\u001b[0m ret \u001b[39m=\u001b[39m eval_class(gt_annotations, dt_annotations, current_classes, difficulties, \u001b[39m0\u001b[39;49m,\n\u001b[0;32m    648\u001b[0m                  min_overlaps, compute_aos, custom_method\u001b[39m=\u001b[39;49mcustom_method)\n\u001b[0;32m    650\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmAP Image BBox finished\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    651\u001b[0m mAP_bbox \u001b[39m=\u001b[39m get_m_ap(ret[\u001b[39m\"\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32me:\\github\\view-of-delft-dataset\\view-of-delft-dataset\\vod\\evaluation\\kitti_official_evaluate.py:533\u001b[0m, in \u001b[0;36meval_class\u001b[1;34m(gt_annotations, dt_annotations, current_classes, difficulties, metric, min_overlaps, compute_aos, num_parts, custom_method)\u001b[0m\n\u001b[0;32m    530\u001b[0m split_parts \u001b[39m=\u001b[39m get_split_parts(num_examples, num_parts)\n\u001b[0;32m    532\u001b[0m \u001b[39m# iou is calculated in \"parts = cluster of frames\"\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m rets \u001b[39m=\u001b[39m calculate_iou_partly(dt_annotations, gt_annotations, metric, num_parts)\n\u001b[0;32m    534\u001b[0m overlaps, parted_overlaps, total_dt_num, total_gt_num \u001b[39m=\u001b[39m rets\n\u001b[0;32m    536\u001b[0m N_SAMPLE_PTS \u001b[39m=\u001b[39m \u001b[39m41\u001b[39m  \u001b[39m# what is this number\u001b[39;00m\n",
      "File \u001b[1;32me:\\github\\view-of-delft-dataset\\view-of-delft-dataset\\vod\\evaluation\\kitti_official_evaluate.py:403\u001b[0m, in \u001b[0;36mcalculate_iou_partly\u001b[1;34m(gt_annotations, dt_annotations, metric, num_parts)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"fast iou algorithm. this function can be used independently to\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[39mdo result analysis. Must be used in CAMERA coordinate system.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39m    num_parts: int. a parameter for fast calculate algorithm\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(gt_annotations) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(dt_annotations)\n\u001b[1;32m--> 403\u001b[0m total_dt_num \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mstack([\u001b[39mlen\u001b[39;49m(a[\u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m]) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m dt_annotations], \u001b[39m0\u001b[39;49m)\n\u001b[0;32m    404\u001b[0m total_gt_num \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack([\u001b[39mlen\u001b[39m(a[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m gt_annotations], \u001b[39m0\u001b[39m)\n\u001b[0;32m    405\u001b[0m num_examples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(gt_annotations)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Realme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\shape_base.py:422\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    420\u001b[0m arrays \u001b[39m=\u001b[39m [asanyarray(arr) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m    421\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m arrays:\n\u001b[1;32m--> 422\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mneed at least one array to stack\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    424\u001b[0m shapes \u001b[39m=\u001b[39m {arr\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays}\n\u001b[0;32m    425\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shapes) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "from vod.evaluation import Evaluation\n",
    "import os\n",
    "\n",
    "# When the instance is created, the label locations are required.\n",
    "#evaluation = Evaluation(test_annotation_file=os.path.join('example_set', 'label'))\n",
    "evaluation = Evaluation(test_annotation_file=os.path.join('E:/vod/view_of_delft_PUBLIC', 'label'))\n",
    "\n",
    "# Using the evaluate method, the model can be evaluated on the detection labels.\n",
    "results = evaluation.evaluate(\n",
    "    #result_path=os.path.join('example_set', 'detection'),\n",
    "    result_path=os.path.join('E:/vod/view_of_delft_PUBLIC/', 'detection'),\n",
    "    current_class=[0, 1, 2])\n",
    "\n",
    "print(\"Results: \\n\"\n",
    "      f\"Entire annotated area: \\n\"\n",
    "      f\"Car: {results['entire_area']['Car_3d_all']} \\n\"\n",
    "      f\"Pedestrian: {results['entire_area']['Pedestrian_3d_all']} \\n\"\n",
    "      f\"Cyclist: {results['entire_area']['Cyclist_3d_all']} \\n\"\n",
    "      f\"mAP: {(results['entire_area']['Car_3d_all'] + results['entire_area']['Pedestrian_3d_all'] + results['entire_area']['Cyclist_3d_all']) / 3} \\n\"\n",
    "      f\"Driving corridor area: \\n\"\n",
    "      f\"Car: {results['roi']['Car_3d_all']} \\n\"\n",
    "      f\"Pedestrian: {results['roi']['Pedestrian_3d_all']} \\n\"\n",
    "      f\"Cyclist: {results['roi']['Cyclist_3d_all']} \\n\"\n",
    "      f\"mAP: {(results['roi']['Car_3d_all'] + results['roi']['Pedestrian_3d_all'] + results['roi']['Cyclist_3d_all']) / 3} \\n\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in c:\\users\\realme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.56.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\realme\\appdata\\roaming\\python\\python310\\site-packages (from numba) (67.6.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.18 in c:\\users\\realme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from numba) (1.23.5)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\users\\realme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from numba) (0.39.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numba --user --upgrade"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
